\section{Classifiers Implementation}\label{sec:classifiers}

Following the selection of the dataset, the \gls{DNA} sequence records were randomly rearranged and assigned to the training and test/validation sets. A suitable data preprocessing strategy was selected in order to turn the \gls{DNA} sequences into a numerical representation. This format was necessary to comply with the input requirements of the classification model, which takes only numerical data.

At this stage, there will be either a collection of pre-calculated features, the descriptors, or encoded training data. It is worth noting that if the data is in descriptor form, only shallow \gls{ML} models will be able to train on it, while \gls{DL} models need it to be in encoding form.

\subsection{Models}

The following sections will provide an insight for each one of the implemented classification models. The process of obtaining this list of models was based on the most common choices made by the authors of the previous studies on \gls{DNA} classification (Table~\ref{tab:previous_work}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{models_feature_extraction}
    \caption{Models and their feature extraction methods.}
    \label{fig:models_feature_extraction}
\end{figure}

\paragraph{MLP}

The first model is a feedforward artificial neural network called \gls{MLP}. This is the only model that can take the descriptors as input data, since it is only the shallow \gls{ML} model implemented. Figure~\ref{fig:mlp-arch} shows its architecture, which was based in a~\citeauthor{Zhang2020DeepHE:Learning}'s research~\cite{Zhang2020DeepHE:Learning}, one of the cases studies of this thesis.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{mlp-arch}
    \caption{MLP architecture.}
    \label{fig:mlp-arch}
\end{figure}

\paragraph{CNN}

The \gls{CNN} model, which was also inspired by one of this thesis' case studies, specifically~\citeauthor{Zou2018AGenomics}'s research~\cite{Zou2018AGenomics} and is depicted in Figure~\ref{fig:cnn-arch}. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{cnn-arch}
    \caption{CNN architecture.}
    \label{fig:cnn-arch}
\end{figure}

\paragraph{LSTM / BiLSTM}

This \gls{LSTM} model is a simple model regarding the number of layers, but it is possible to pass to the \textit{nn.LSTM} layer a parameter called $num\_layers$ that specifies the number of recurrent layers. Setting $num \_layers = 2$ would result in a stacked \gls{LSTM}, which would consist of two \gls{LSTM}s stacked on top of one another. The second \gls{LSTM} would receive input from the first and compute the final results. Besides, this layer can also take an argument called $bidirectional$, which determines if the \gls{LSTM} is bidirectional or not. This layer can also take a $dropout$ argument that introduces a dropout layer on the outputs of each \gls{LSTM} layer except the last one.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{lstm-arch}
    \caption{LSTM architecture.}
    \label{fig:lstm-arch}
\end{figure}

\paragraph{GRU / BiGRU} This \gls{GRU} model is identical to the \gls{LSTM}'s, only changing from the \textit{nn.LSTM} layer to \textit{nn.GRU} one. This way it is possible to directly compare these two types of recurrent neural networks.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{gru-arch}
    \caption{GRU architecture.}
    \label{fig:gru-arch}
\end{figure}

\paragraph{CNN-LSTM / CNN-BiLSTM}

This model is a combination of the \gls{CNN} and \gls{LSTM} models. When using the \gls{CNN} model, the output of the \gls{CNN} is fed into the \gls{LSTM} model. The same previously mentioned properties of \gls{LSTM} are also present (using $num\_layers$ to create a stacked \gls{LSTM} and using $dropout$ to introduce dropout layers on the outputs of each \gls{LSTM} layers except the last one).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{cnn-lstm-arch}
    \caption{CNN-LSTM architecture.}
    \label{fig:cnn-lstm-arch}
\end{figure}

\paragraph{CNN-GRU / CNN-BiGRU}

This model is a combination of the \gls{CNN} and \gls{GRU} models. When using the \gls{CNN} model, the output of the \gls{CNN} is fed into the \gls{GRU} model. The same previously mentioned properties of \gls{GRU} are also present (using $num\_layers$ to create a stacked \gls{GRU} and using $dropout$ to introduce dropout layers on the outputs of each \gls{GRU} layers except the last one).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{cnn-gru-arch}
    \caption{CNN-GRU architecture.}
    \label{fig:cnn-gru-arch}
\end{figure}

\subsection{Hyperparameter Tuning}

Additionally, hyperparameter optimization was considered and applied successfully. As mentioned in Section~\ref{sec:workflow}, the challenge of hyperparameter optimization is selecting the appropriate hyperparameters for a learning algorithm. It may distinguish an ordinary model from a very accurate one. Choosing a different learning rate or modifying the size of a network layer may have a substantial effect on the performance of the model. Ray Tune\footnote{\href{https://docs.ray.io/en/latest/tune/}{https://docs.ray.io/en/latest/tune/}} is a standard tuning tool for hyperparameters~\cite{Liaw2018Tune:Training} and it was used to complete this task. However, before finding the best combination, it is required to define the configuration of the Ray Tune's search space. The implemented one can be found in Table~\ref{tab:search_space}.

\begin{table}[ht]
	\caption{Ray Tune's search space.}
	\label{tab:search_space}
    \centering
    \begin{tabular}{lll}
    	\toprule
    	\textbf{Hyperparameter (x)} & \textbf{Search Space} & \textbf{Models} \\\midrule
    	
    	Hidden Size & $x \in \{32, 64, 128\}$ & All\\\midrule
        Batch Size & $x \in \{16, 32, 64\}$ & All\\\midrule
        Learning Rate & $x \in \{0.0001, 0.001, 0.01\}$ & All\\\midrule
        Dropout & $x \in \{0.2, 0.3, 0.4, 0.5\}$ & All\\\midrule
        Number of Layers & $x \in \{1, 2, 3\}$ & All except MLP and CNN\\
        
    	\bottomrule
    \end{tabular}
\end{table}

Ray Tune will randomly choose a combination of parameters from these search areas for each trial. It will then train many models in parallel and determine which has the highest performance, according to a defined metric. Additionally, the Ray Tune's scheduler \textit{HyperBandScheduler} was used, which terminates poorly performing trials early, using the HyperBand optimization algorithm. This is accomplished by choosing a desired metric (loss) and measuring it at the end of each epoch. If the measure keeps worsening, reaching a specified patience value, the trial will end immediately. This strategy was also implemented in the case when hyperparameter tuning is not being performed, using now the \textit{ReduceLROnPlateau} scheduler from \textit{PyTorch}. 